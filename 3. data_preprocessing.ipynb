{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import src.utils as utils\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Config File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DATA = utils.config_load()\n",
    "CONFIG_DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary EDA**:\n",
    "- No missing value\n",
    "- Features are uncorellated\n",
    "- Scaling for Time and Amount Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(return_file=True):\n",
    "    # Load train data\n",
    "    X_train = utils.pickle_load(CONFIG_DATA['train_set_path'][0])\n",
    "    y_train = utils.pickle_load(CONFIG_DATA['train_set_path'][1])\n",
    "\n",
    "    # Load valid data\n",
    "    X_valid = utils.pickle_load(CONFIG_DATA['valid_set_path'][0])\n",
    "    y_valid = utils.pickle_load(CONFIG_DATA['valid_set_path'][1])\n",
    "\n",
    "    # Load test data\n",
    "    X_test = utils.pickle_load(CONFIG_DATA['test_set_path'][0])\n",
    "    y_test = utils.pickle_load(CONFIG_DATA['test_set_path'][1])\n",
    "\n",
    "    # Print \n",
    "    print(\"X_train shape :\", X_train.shape)\n",
    "    print(\"y_train shape :\", y_train.shape)\n",
    "    print(\"X_valid shape :\", X_valid.shape)\n",
    "    print(\"y_valid shape :\", y_valid.shape)\n",
    "    print(\"X_test shape  :\", X_test.shape)\n",
    "    print(\"y_test shape  :\", y_test.shape)\n",
    "\n",
    "    if return_file:\n",
    "        return X_train, X_valid, X_test, y_train, y_valid, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test = load_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_standardize(data, return_file=True, columns=['Time', 'Amount']):\n",
    "    \"\"\"Find standardizer data\"\"\"\n",
    "    standardizer = RobustScaler()\n",
    "\n",
    "    # Fit standardizer\n",
    "    standardizer.fit(data[columns])\n",
    "\n",
    "    # Dump standardizer\n",
    "    utils.pickle_dump(standardizer, CONFIG_DATA['standardizer_path'])\n",
    "    \n",
    "    if return_file:\n",
    "        return standardizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit standardizer\n",
    "standardizer = fit_standardize(data=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_standardize(data, standardizer, columns=['Time', 'Amount']):\n",
    "    \"\"\"Function to standardize data\"\"\"\n",
    "    data_standard = pd.DataFrame(standardizer.transform(data[columns]))\n",
    "    data_standard.index = data.index\n",
    "    data[columns] = data_standard\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform\n",
    "X_train_std = transform_standardize(data = X_train,\n",
    "                                    standardizer = standardizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See that the data is unbalanced\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the downsampling (only for training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_undersampler(X, y):\n",
    "    \"\"\"Function to under sample the majority data\"\"\"\n",
    "    # Create resampling object\n",
    "    ros = RandomUnderSampler(random_state = CONFIG_DATA['seed'])\n",
    "\n",
    "    # Balancing the set data\n",
    "    X_resample, y_resample = ros.fit_resample(X, y)\n",
    "\n",
    "    # Print\n",
    "    print('Distribution before resampling :')\n",
    "    print(y.value_counts())\n",
    "    print(\"\")\n",
    "    print('Distribution after resampling  :')\n",
    "    print(y_resample.value_counts())\n",
    "\n",
    "    return X_resample, y_resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean, y_train_clean = random_undersampler(X_train_std, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = {\n",
    "    'standardizer': standardizer\n",
    "}\n",
    "\n",
    "utils.pickle_dump(preprocessor, CONFIG_DATA['preprocessor_path'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(data,  standardizer):\n",
    "    \"\"\"Function to clean data\"\"\"\n",
    "\n",
    "    # Standardize data\n",
    "    data_standard = transform_standardize(data, standardizer)\n",
    "\n",
    "    return data_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess_data(data):\n",
    "    \"\"\"Function to preprocess data\"\"\"\n",
    "    # Load preprocessor\n",
    "    preprocessor = utils.pickle_load(CONFIG_DATA['preprocessor_path'])\n",
    "    standardizer = preprocessor['standardizer']\n",
    "\n",
    "    data_clean = clean_data(data,\n",
    "                            standardizer)\n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess all"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preprocessor(return_file=True):\n",
    "    \"\"\"Function to generate preprocessor\"\"\"\n",
    "    # Load data\n",
    "    X = utils.pickle_load(CONFIG_DATA['train_set_path'][0])\n",
    "    y = utils.pickle_load(CONFIG_DATA['train_set_path'][1])\n",
    "\n",
    "    # Generate preprocessor: standardizer\n",
    "    standardizer = fit_standardize(X)\n",
    "\n",
    "    # Dump file\n",
    "    preprocessor = {\n",
    "        'standardizer': standardizer\n",
    "    }\n",
    "    utils.pickle_dump(preprocessor, CONFIG_DATA['preprocessor_path'])\n",
    "    \n",
    "    if return_file:\n",
    "        return preprocessor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = generate_preprocessor()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For X train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(type='train', return_file=True):\n",
    "    \"\"\"Function to preprocess train data\"\"\"\n",
    "    # Load data\n",
    "    X = utils.pickle_load(CONFIG_DATA[f'{type}_set_path'][0])\n",
    "    y = utils.pickle_load(CONFIG_DATA[f'{type}_set_path'][1])\n",
    "        \n",
    "    # Preprocess data\n",
    "    X_clean = _preprocess_data(X)\n",
    "    y_clean = y\n",
    "\n",
    "    # FOR TRAINING ONLY -> DO UNDERSAMPLING\n",
    "    if type == 'train':\n",
    "        X_clean, y_clean = random_undersampler(X_clean, y_clean)\n",
    "\n",
    "    # Print shape\n",
    "    print(\"X clean shape:\", X_clean.shape)\n",
    "    print(\"y clean shape:\", y_clean.shape)\n",
    "\n",
    "    # Dump file\n",
    "    utils.pickle_dump(X_clean, CONFIG_DATA[f'{type}_clean_path'][0])\n",
    "    utils.pickle_dump(y_clean, CONFIG_DATA[f'{type}_clean_path'][1])\n",
    "\n",
    "    if return_file:\n",
    "        return X_clean, y_clean    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train\n",
    "X_train_clean, y_train_clean = preprocess_data(type = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_valid\n",
    "X_valid_clean, y_valid_clean = preprocess_data(type = 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X_train\n",
    "X_test_clean, y_test_clean = preprocess_data(type = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clean.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
